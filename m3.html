<html>
	<head>
	 <title>Bootstrap Example</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
    <link rel="stylesheet" href="css/style.css">
      	<link rel="stylesheet" href="/storage/emulated/0/javaprograms/Hello1(jq)/jquery-ui/jquery-ui.css">
      	<link rel="stylesheet" href="/storage/emulated/0/javaprograms/Hello1(jq)/jquery-ui/jquery-ui.structure.css">
      <link rel="stylesheet" href="/storage/emulated/0/javaprograms/Hello1(jq)/jquery-ui/jquery-ui.theme.css">
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
		<style>
			.color3
			{
				height:700px;
			}
			div.t
			{
				line-height:200%
			}
			.yy
			{
				text-align:center;
			}
			div.tp
			{
				line-height:200%
			}
			.ba
			{
				width:200px;
				font-size:50px;
			}
		</style>
		
		</head>
		<body>
			<div class="container-fluid">
			<div class="col-sm-12 col-md-8 color2">
			<img src="SAVE_20200916_172959.jpg" class="color3 img-responsive"/>
		<h1>ABOUT THIS COURSE</h1>	
		<h3><div class="t">

This intermediate-level course introduces the mathematical foundations to derive Principal Component Analysis (PCA), a fundamental dimensionality reduction technique. We'll cover some basic statistics of data sets, such as mean values and variances, we'll compute distances and angles between vectors using inner products and derive orthogonal projections of data onto lower-dimensional subspaces. Using all these tools, we'll then derive PCA as a method that minimizes the average squared reconstruction error between data points and their reconstruction

</div></h3>
<div>
<h1>Syllabus - What you will learn from this course</h1>

<h1 class="yy">WEEK 1</h1>

<h1>Statistics of Datasets





</h1>
<h3>
<div class="tp">
Principal Component Analysis (PCA) is one of the most important dimensionality reduction algorithms in machine learning. In this course, we lay the mathematical foundations to derive and understand PCA from a geometric point of view. In this module, we learn how to summarize datasets (e.g., images) using basic statistics, such as the mean and the variance. We also look at properties of the mean and the variance when we shift or scale the original data set. We will provide mathematical intuition as well as the skills to derive the results. We will also implement our results in code (jupyter notebooks), which will allow us to practice our mathematical understand to compute averages of image data sets.




</div></h3>

<h1 style="text-align:center">WEEK 2</h1>

<h1 >Inner Products





</h1>
<h3>
<div class="t">
Data can be interpreted as vectors. Vectors allow us to talk about geometric concepts, such as lengths, distances and angles to characterise similarity between vectors. This will become important later in the course when we discuss PCA. In this module, we will introduce and practice the concept of an inner product. Inner products allow us to talk about geometric concepts in vector spaces. More specifically, we will start with the dot product (which we may still know from school) as a special case of an inner product, and then move toward a more general concept of an inner product, which play an integral part in some areas of machine learning, such as kernel machines (this includes support vector machines and Gaussian processes). We have a lot of exercises in this module to practice and understand the concept of inner products.
</div>
</h3>
	</div>
			</div>
			<div class="container text-center ">
			<a href="successful.html" class="btn btn-success ba">Enroll</a>
			</div>
			</body>
</html>
